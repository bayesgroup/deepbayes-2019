{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_bayes_SS_VAE_blank.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laSwDfOZx3RT",
        "colab_type": "text"
      },
      "source": [
        "*(to use GPU in colab go to Runtime -> Change Runtime Type and change the hardware accelerator)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVGY58iGt0_f",
        "colab_type": "text"
      },
      "source": [
        "# VAE with Discrete Variables For Semi-Supervised Learning\n",
        "\n",
        "This practical session is inspired by [\"Semi-supervised Learning with\n",
        "Deep Generative Models\"](https://arxiv.org/pdf/1406.5298.pdf). We will also use this model to illustrate the Gumbel-Softmax trick."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVmzR0wRtbD1",
        "colab_type": "code",
        "outputId": "1953e8ff-7c44-4165-b9c8-09075fa1dc79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "from torch.distributions import Normal, Bernoulli, Independent\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "print('Using torch version {}'.format(torch.__version__))\n",
        "print('Using {} device'.format(device))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using torch version 1.1.0\n",
            "Using cuda:0 device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m67VBfpqthA2",
        "colab_type": "text"
      },
      "source": [
        "For the semi-supervised learning task we remove 95% of labels from the training set. In the modified training set the observed labels have a standard one-hot encoding and the unobserved labels are represented by all-zero ten dimensional vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrKF-qMPthXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = MNIST(root='.', download=True, train=True)\n",
        "new_train_labels = torch.zeros(60000, 10)\n",
        "observed = np.random.choice(60000, 3000)\n",
        "new_train_labels[observed] = torch.eye(10)[data.targets][observed]\n",
        "train_data = TensorDataset(data.data.view(-1, 28 * 28).float() / 255,\n",
        "                           new_train_labels)\n",
        "\n",
        "data = MNIST(root='.', download=True, train=False)\n",
        "test_data = TensorDataset(data.data.view(-1, 28 * 28).float() / 255,\n",
        "                          data.targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej9Z1YEZt-Rw",
        "colab_type": "text"
      },
      "source": [
        "## The probabilistic model\n",
        "\n",
        "In the semi-supervised setting, the generative model is a little more complicated. In particular, it incorporates a new variable $y$ that represents the digits class.\n",
        "\n",
        "\\begin{align*}\n",
        "& p(x, y, z) = p(x \\mid y, z) p(z) p(y) \\\\\n",
        "& p(y) = Cat(y \\mid \\pi_0), \\pi_0 = (1/10, \\dots, 1/10) \\\\\n",
        "& p(z) = \\mathcal N(z \\mid 0, I) \\\\\n",
        "& p(x \\mid y, z) = \\prod_{i=1}^D p_i(y, z)^{x_i} (1 - p_i(y, z))^{1 - x_i}\n",
        "\\end{align*}\n",
        "\n",
        "Typically, whenever we train a model with partial observations, we interpret unobserved variables as latent variables and marginalize over them. In this case, the loss function splits into two terms: one for observed variables (we denote the set of indices of observed labels $P$), another for unobserved.\n",
        "\n",
        "\\begin{equation}\n",
        "L(X, y) = \\sum_{i \\notin P} \\log p(x_i) + \\sum_{i \\in P} \\log p(x_i, y_i)\n",
        "\\end{equation}\n",
        "\n",
        "Again, we can't compute the exact values of marginal likelihoods and resort to variational lower bound on likelihood. To compute lower bounds we define the following variational approximation:\n",
        "\n",
        "\\begin{align*}\n",
        "& q(y, z \\mid x) = q(y \\mid x) q(z \\mid y, x)\\\\\n",
        "& \\\\\n",
        "& q(y \\mid x) = Cat(y \\mid \\pi(x))\\\\\n",
        "& q(z \\mid y, x) = \\mathcal N(z \\mid \\mu_\\phi(x, y), \\operatorname{diag}\\sigma^2_\\phi(y, x))\n",
        "\\end{align*}\n",
        "\n",
        "### ELBO for observed variables\n",
        "\n",
        "Similiar to VAE:\n",
        "\n",
        "\\begin{equation}\n",
        "\\log p(x, y) = \\log \\mathbb E_{p(z)} p(x, y \\mid z) \\geq \\mathbb E_{q(z \\mid y, x)} \\log \\frac{p(x, y \\mid z) p(z)}{q(z \\mid y, x)}\n",
        "\\end{equation}\n",
        "\n",
        "### ELBO for unobserved variables\n",
        "\n",
        "\\begin{equation}\n",
        "\\log p(x) = \\log \\mathbb E_{p(y)} \\mathbb E_{p(z \\mid y)} \\log p(x\\mid z, y)\\geq \\mathbb E_{q(y \\mid x)} \\mathbb E_{q(z \\mid y, x)} \\log \\frac{p(x, y \\mid z) p(z)}{q(z \\mid y, x) q(y \\mid x)}\n",
        "\\end{equation}\n",
        "\n",
        "### The final objective\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal L(X, y) = \\sum_{i \\in P} \\mathbb E_{q(z_i \\mid y_i, x_i)} \\log \\frac{p(x_i, y_i \\mid z_i) p(z_i)}{q(z_i \\mid y_i, x_i)} + \\sum_{i \\notin P} \\mathbb E_{q(y_i \\mid x_i)} \\mathbb E_{q(z_i \\mid y_i, x_i)} \\log \\frac{p(x_i, y_i \\mid z_i) p(z_i)}{q(z_i \\mid y_i, x_i) q(y_i \\mid x_i)}\n",
        "\\end{equation}\n",
        "\n",
        "Again, we will use reparametrized Monte-Carlo estimates to approximate expectation over $z$. To approximate expectaion over discrete variables $y$ we will use Gumbel-Softmax trick.\n",
        "\n",
        "## Important practical aspect\n",
        "\n",
        "ELBO maximization does not lead to any semantics in latent variables $y$. \n",
        "\n",
        "We are going to restrict variational approximations $q(y \\mid x)$ to the ones that correctly classify observation $x$ on fully-observed variables $(x_i, y_i)$. As in the original paper, we will add a cross-entropy regularizer to the objective with weight $\\alpha$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{1}{|P|}\\sum_{i \\in P} y_i^T \\log q(y \\mid x).\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvS1V7eRuVFs",
        "colab_type": "text"
      },
      "source": [
        "## RelaxedOneHotCategorical\n",
        "\n",
        "In the probabilistic model defined above we are going to replace categorical prior $p(y)$ and categorical variational approximation $q(y | x)$ with Gumbel-Softmax distribution. The distribution class is implemented in **torch.distributions.relaxed_categorical.RelaxedOneHotCategorical**.\n",
        "\n",
        "For more details see [Categorical Reparameterization with Gumbel-Softmax](https://arxiv.org/abs/1611.01144).\n",
        "\n",
        "### An illustration for Gumbel-Softmax\n",
        "\n",
        "- Temperature allows for smooth interpolation between one-hot categorical distribution with low temperature and a $(1/K, \\dots, 1/K)$ vector with high temperatures\n",
        "- The exact computation of $\\mathbb E_{q(y|x)} f(y)$ requires computation of $f(y)$ for ten possible labels $y=0, \\dots, 9$. On the other hand, with Gumbel-Softmax relaxation only one sample $y \\sim q(y | x)$ is enough. Therefore, Gumbel-Softmax gives almost a ten-fold training speed increase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zreLi7xnuVby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.cm as cm\n",
        "from torch.distributions.relaxed_categorical import RelaxedOneHotCategorical\n",
        "\n",
        "n_classes = 4\n",
        "logits = torch.randn(1, n_classes)\n",
        "print('Probs: ', torch.nn.functional.softmax(logits, 1).squeeze().numpy())\n",
        "temperatures = [0.1, 0.5, 1., 5., 10.]\n",
        "M = 128 # number of samples used to approximate distribution mean\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=len(temperatures), figsize=(14, 6),\n",
        "                         subplot_kw={'xticks': range(n_classes),\n",
        "                                     'yticks': [0., 0.5, 1.]})\n",
        "axes[0, 0].set_ylabel('Expectation')\n",
        "axes[1, 0].set_ylabel('Gumbel Softmax Sample')\n",
        "\n",
        "for n, t in enumerate(temperatures):\n",
        "    dist = RelaxedOneHotCategorical(t, logits=logits)\n",
        "    mean = torch.zeros_like(logits)\n",
        "    for _ in range(M):\n",
        "        mean += dist.sample() / M\n",
        "    sample = dist.sample()\n",
        "    \n",
        "    axes[0, n].set_title('T = {}'.format(t))\n",
        "    axes[0, n].set_ylim((0, 1.1))\n",
        "    axes[1, n].set_ylim((0, 1.1))\n",
        "    axes[0, n].bar(np.arange(n_classes), mean.numpy().reshape(n_classes),\n",
        "                   color=cm.plasma(0.75 * t / max(temperatures)))\n",
        "    axes[1, n].bar(np.arange(n_classes), sample.numpy().reshape(n_classes),\n",
        "                   color=cm.plasma(0.75 * t / max(temperatures)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdgeSVo0uh8M",
        "colab_type": "text"
      },
      "source": [
        "# SS-VAE implementation\n",
        "\n",
        "The computational graph for observed labels has the following structure:\n",
        "\n",
        "![computational graph ss vae xy](https://github.com/bayesgroup/deepbayes-2018/blob/master/day2_vae/ss_vae_xy.png?raw=true)\n",
        "\n",
        "The computational graph for unobserved lables has the following structure:\n",
        "\n",
        "![computational graph ss vae xy](https://github.com/bayesgroup/deepbayes-2018/blob/master/day2_vae/ss_vae_x.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnXphotoubKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_classes, d, nh, D = 10, 32, 500, 28 * 28\n",
        "default_T = torch.tensor(0.6, device=device)\n",
        "\n",
        "yz_dec = nn.Sequential(\n",
        "    nn.Linear(n_classes + d, nh),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(nh, D))\n",
        "\n",
        "y_enc = nn.Sequential(\n",
        "    nn.Linear(D, nh),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(nh, n_classes))\n",
        "\n",
        "z_enc = nn.Sequential(\n",
        "    nn.Linear(n_classes + D, nh),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(nh, 2 * d)\n",
        "    )\n",
        "\n",
        "yz_dec = yz_dec.to(device)\n",
        "y_enc = y_enc.to(device)\n",
        "z_enc = z_enc.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxATI2R-unLl",
        "colab_type": "text"
      },
      "source": [
        "# The task\n",
        "\n",
        "Implement the loss function for the semi-supervised variational autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDY8iEDoumTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(x, y, y_encoder, z_encoder, decoder, T=default_T, alpha=32.):#, verbose=False):\n",
        "    #TODO\n",
        "    \"\"\"\n",
        "    NOTE:                                                                        \n",
        "      hyperparameter alpha was tuned for the implementation that computed\n",
        "      the mean of elbo terms and sum of cross-entropy terms over the observed\n",
        "      datapoints in the batch  \n",
        "                                                                                 \n",
        "      In the modified training set the observed labels have a standard one-hot     \n",
        "      encoding and the unobserved labels are represented by all-zero ten           \n",
        "      dimensional vectors.                                                         \n",
        "      To compute the mask for observed labels you can compute                      \n",
        "      y_is_observed = y.sum(1, keepdim=True)                                       \n",
        "    \n",
        "    The function has to\n",
        "    1. sample y from q(y | x)\n",
        "    2. sample z from q(z | x, y)\n",
        "    3. compute the evidence lower bound for obervsed and unobserved variables\n",
        "    4. compute the cross_entropy regularizer with weight alpha for object with\n",
        "      observed labels\n",
        "    5. return the sum of two losses\n",
        "    \"\"\"\n",
        "    pass\n",
        "    #return loss + alpha * loss_supervised"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3U3fEiAuqQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "def train_model(y_encoder, z_encoder, decoder, batch_size=100, num_epochs=3, learning_rate=1e-3):\n",
        "    gd = optim.Adam(chain(y_encoder.parameters(),\n",
        "                          z_encoder.parameters(),\n",
        "                          decoder.parameters()), lr=learning_rate)\n",
        "    dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "    train_losses = []\n",
        "    for _ in range(num_epochs):\n",
        "        for i, (x, y) in enumerate(dataloader):\n",
        "            total = len(dataloader)\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            loss_value = loss(x, y, y_encoder, z_encoder, decoder)\n",
        "            (-loss_value).backward()\n",
        "            train_losses.append(loss_value.cpu().item())\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print('\\rTrain loss:', train_losses[-1],\n",
        "                      'Batch', i + 1, 'of', total, ' ' * 10, end='', flush=True)\n",
        "            gd.step()\n",
        "            gd.zero_grad()\n",
        "        loss_value = 0.\n",
        "        accuracy = 0.\n",
        "        for i, (x, y) in enumerate(test_dataloader):\n",
        "            total = len(test_dataloader)\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            unobserved_y = torch.zeros((y.shape[0], 10)).to(device)\n",
        "            loss_value += loss(x, unobserved_y, y_encoder, z_encoder, decoder).item()\n",
        "            accuracy += (torch.argmax(y_encoder(x), 1) == y).double().mean().item()\n",
        "        print('Test loss: {}\\t Test accuracy: {}'.format(loss_value / total, accuracy / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwAL5ZtXutfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# my implementation omitted log p(y) for observed variables. it has\n",
        "# test loss -106.79\n",
        "# test accuracy 0.95\n",
        "train_model(y_enc, z_enc, yz_dec, num_epochs=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ifxO95Puxt_",
        "colab_type": "text"
      },
      "source": [
        "## Visualizations\n",
        "\n",
        "Generate 10 images for each label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEA9Z6x-u067",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_samples_with_fixed_classes(dec):\n",
        "    decoder_input = torch.cat((torch.eye(10).repeat(10, 1), torch.randn(100, d)), 1)\n",
        "    decoder_input = decoder_input.to(device)\n",
        "    images = torch.sigmoid(dec(decoder_input)).view(100, 28, 28).detach().cpu().numpy()\n",
        "    \n",
        "    fig, axes = plt.subplots(nrows=10, ncols=10, figsize=(14, 14),\n",
        "                             subplot_kw={'xticks': [], 'yticks': []})\n",
        "    for i in range(10):\n",
        "        axes[0, i].set_title('{}'.format(i))\n",
        "    \n",
        "    for i in range(100):\n",
        "        axes[int(i / 10), i % 10].imshow(images[i], cmap='gray')\n",
        "        \n",
        "plot_samples_with_fixed_classes(yz_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_NdhbjEu3lL",
        "colab_type": "text"
      },
      "source": [
        "### \"Style-transfer\"\n",
        "\n",
        "Here we infer latent representation $z$ of a given digit $x$ and then generate from $p(x | z, y)$ for different $y$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B8XBBp9u4Yn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_all_digits_with_fixed_style(z_enc, y_enc, dec):\n",
        "    indices = np.random.choice(10000, 10)\n",
        "    x, y = test_data[indices][0], torch.eye(10)[test_data[indices][1]]\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    z = z_enc(torch.cat((x, y), 1))[:, :d]\n",
        "\n",
        "    # generate digits\n",
        "    images = []\n",
        "    for i in range(10):\n",
        "        digit_encodings = torch.eye(10)[i, :].expand(10, 10).to(device)\n",
        "        images.append(torch.sigmoid(dec(torch.cat((digit_encodings, z), 1)).view(10, 28, 28)).detach().cpu().numpy())\n",
        "        \n",
        "    x = x.view(10, 28, 28).detach().cpu().numpy()\n",
        "\n",
        "    # plot\n",
        "    fig, axes = plt.subplots(nrows=10, ncols=11, figsize=(14, 14),\n",
        "                             subplot_kw={'xticks': [], 'yticks': []})\n",
        "    \n",
        "    axes[0, 0].set_title('example')\n",
        "    for i in range(10):\n",
        "        axes[0, i + 1].set_title('{}'.format(i))\n",
        "        axes[i, 0].imshow(x[i], cmap='gray')\n",
        "        for j in range(10):\n",
        "            axes[i, j + 1].imshow(images[j][i], cmap='gray')\n",
        "            \n",
        "plot_all_digits_with_fixed_style(z_enc, y_enc, yz_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFQXhqT3u9Kf",
        "colab_type": "text"
      },
      "source": [
        "### T-SNE for SS-VAE\n",
        "\n",
        "Do you notice any difference from T-SNE for vanilla VAE? How can you interpret the results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJaqQ9KyvAVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_tsne(objects, labels):\n",
        "    from sklearn.manifold import TSNE\n",
        "    embeddings = TSNE(n_components=2).fit_transform(objects)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    for k in range(10):\n",
        "        embeddings_for_k = embeddings[labels == k]\n",
        "        plt.scatter(embeddings_for_k[:, 0], embeddings_for_k[:, 1],\n",
        "                    label='{}'.format(k))\n",
        "    plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxmfeI34vCKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# T-SNE for q(z | x, y) mean\n",
        "labels = test_data[:1000][1].numpy()\n",
        "encoder_input = torch.cat((test_data[:1000][0],\n",
        "                           torch.eye(10)[labels]), 1).to(device)\n",
        "latent_variables = z_enc(encoder_input)[:, :d]\n",
        "latent_variables = latent_variables.detach().cpu().numpy()\n",
        "\n",
        "plot_tsne(latent_variables, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVpp-hhwvHvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# T-SNE for q(y | x) logits\n",
        "labels = test_data[:1000][1].numpy()\n",
        "latent_variables = y_enc(test_data[:1000][0].to(device))\n",
        "latent_variables = latent_variables.detach().cpu().numpy()\n",
        "\n",
        "plot_tsne(latent_variables, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm_7HjqM3VXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}