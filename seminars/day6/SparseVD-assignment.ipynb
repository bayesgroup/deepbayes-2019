{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"SparseVD-assignment.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"gaNt3kQaJnSw","colab_type":"text"},"source":["# Assignment 3"]},{"cell_type":"markdown","metadata":{"id":"biSNbjH4JnSy","colab_type":"text"},"source":["# Variational Dropout Sparsifies Deep Neural Networks\n","\n","Variational Dropout ([arXiv:1506.02557](https://arxiv.org/abs/1506.02557)) provides a Bayesian interpretation of the conventional dropout procedure. Later it was shown that Variational Dropout can be used for model sparsification (Sparse VD), an the effect can be achieved via optimization of variational lower bound wrt individual dropout rates for every weight of the model ([arXiv:1701.05369](https://arxiv.org/abs/1701.05369)).\n","\n","#### Sparse VD\n","\n","Sparse VD model optimizes VLB $\\mathcal{L}(\\phi)$ with respect to parameters $\\phi$ of a variational approximation $q_\\phi(w)$:\n","\n","#$$\\mathcal{L}(\\phi) =  L_\\mathcal{D}(\\phi) - D_{KL}(q_\\phi(w)\\,\\|\\,p(w)) \\to\\max_{\\phi\\in\\Phi}$$\n","#$$L_\\mathcal{D}(\\phi) = \\sum_{n=1}^N \\mathrm{E}_{q_\\phi(w)}[\\log p(y_n\\,|\\,x_n, w)],$$\n","\n","where $p(w)$ is the log-uniform prior distibution, the variational approximation $q_\\phi(w)$ is a fullly factorized gaussian, the likelihood $p(y\\,|\\,x, w)$ is defined by a neural network with parametrs $w$. The optimization is performed by stochasic optimization methods e.g., Adam, etc.\n","\n","For computational convenience, the KL divergence is approximated as follows:\n","#$$-D_{KL}(q(w_{ij}\\,|\\,\\theta_{ij}, \\alpha_{ij})\\,\\|\\,p(w_{ij})) \\approx$$\n","#$$ \\approx k_1\\sigma(k_2 + k_3\\log \\alpha_{ij})) - 0.5\\log(1+\\alpha_{ij}^{-1}) + \\mathrm{C}$$\n","#$$ k_1=0.63576 \\quad k_2=1.87320 \\quad k_3=1.48695$$\n","\n","\n","**Note:** In the paper two parametrizations of q are used. The fist one is $\\phi_i=\\{\\mu_{i}, \\sigma_i\\}$ that means $w_{ij} \\sim N(w_{ij} | \\mu_{ij}, \\sigma^2_{ij})$ and the second one is $\\phi_{ij}=\\{\\mu_{ij}, \\alpha_{ij}\\}$ that means $w_{ij} \\sim N(w_{ij} | \\mu_{ij}, \\alpha_{ij}\\mu^2_{ij})$. This two parametrization are connected as $\\sigma^2_{ij} = \\alpha_{ij}\\mu^2_{ij}$. Do not be confused.\n","\n","![alt text](https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/images/svd3.png)\n","\n","\n","# In this assignment:\n","1. Implementation of fully-connected Sparse VD layer\n","2. Training Lenet-300-100 on MNIST dataset\n","3. Optional Research Assignment\n","\n","Additional information:\n","- If you have a problem with importing logger, download logger.py and file to the same folder and run a notebook from it\n","- You will need the following python packages: pytorch, numpy, sklearn, pylab (matplotlib), tabulate\n","- If you have an urgent question or find a typo or a mistake, send it to ars.ashuha@gmail.com. The title should include \"BDL Assignment 3, 2019\""]},{"cell_type":"code","metadata":{"id":"NXhZgXYQJnSz","colab_type":"code","colab":{}},"source":["import math\n","import time\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.nn import Parameter\n","from torchvision import datasets, transforms\n","\n","from logger import Logger"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R2V-uQDOJnS2","colab_type":"text"},"source":["## Implementation of  Sparse VD layer"]},{"cell_type":"code","metadata":{"id":"GdxU0K10JnS3","colab_type":"code","colab":{}},"source":["class LinearSVDO(nn.Module):\n","    def __init__(self, in_features, out_features, threshold, bias=True):\n","        super(LinearSVDO, self).__init__()\n","        \"\"\"\n","            in_features: int, a number of input features\n","            out_features: int, a number of neurons\n","            threshold: float, a threshold for clipping weights\n","        \"\"\"\n","        \n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.threshold = threshold\n","\n","        self.mu = # torch.nn.parameter.Parameter of size out_features x in_features\n","        self.log_sigma = # torch.nn.parameter.Parameter of size out_features x in_features\n","        self.bias = # torch.nn.parameter.Parameter of size 1 x out_features\n","        self.reset_parameters()\n","        \n","    def reset_parameters(self):\n","        self.bias.data.zero_()\n","        self.mu.data.normal_(0, 0.02)\n","        self.log_sigma.data.fill_(-5)        \n","        \n","    def forward(self, x):      \n","        # x is a torch.Tensor of shape (number_of_objects, in_features)\n","        # log_alpha is a torch.Tensor of shape (out_features, in_features)\n","        self.log_alpha = # Compute using self.log_sigma and self.mu\n","        # clipping for a numerical stability\n","        self.log_alpha = torch.clamp(self.log_alpha, -10, 10)   \n","        \n","        if self.training:\n","            # LRT = local reparametrization trick\n","            # lrt_mean is a torch.Tensor of shape (x.shape[0], out_features)\n","            lrt_mean =  # compute mean activation using LRT\n","            # lrt_std is a torch.Tensor of shape (x.shape[0], out_features)\n","            lrt_std = # compute std of activations unsig lrt, \n","                      # do not forget use torch.sqrt(x + 1e-8) instead of torch.sqrt(x)\n","            # eps is a torch.Tensor of shape (x.shape[0], out_features)\n","            eps = # sample of noise for reparametrization\n","            return # sample of activation\n","        \n","        out = # compute the output of the layer\n","        # use weights W = E q = self.mu\n","        # clip all weight with log_alpha > threshold\n","        return out\n","        \n","    def kl_reg(self):\n","        k1, k2, k3 = torch.Tensor([0.63576]), torch.Tensor([1.8732]), torch.Tensor([1.48695])\n","        # kl is a scalar torch.Tensor \n","        kl = # eval KL using the approximation\n","        return kl"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ps7_riOmJnS5","colab_type":"text"},"source":["## Define LeNet-300-100"]},{"cell_type":"code","metadata":{"id":"5w7rroIeJnS7","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","    def __init__(self, threshold):\n","        super(Net, self).__init__()\n","        self.fc1 = LinearSVDO(28*28, 300, threshold)\n","        self.fc2 = LinearSVDO(300,  100, threshold)\n","        self.fc3 = LinearSVDO(100,  10, threshold)\n","        self.threshold=threshold\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.log_softmax(self.fc3(x), dim=1)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wlpR1Xr2JnS9","colab_type":"text"},"source":["## Function for loading MNIST"]},{"cell_type":"code","metadata":{"id":"6WWJNZYEJnS_","colab_type":"code","colab":{}},"source":["def get_mnist(batch_size):\n","    trsnform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n","    train_loader = torch.utils.data.DataLoader(\n","        datasets.MNIST('../data', train=True, download=True,\n","        transform=trsnform), batch_size=batch_size, shuffle=True)\n","    test_loader = torch.utils.data.DataLoader(\n","        datasets.MNIST('../data', train=False, download=True,\n","        transform=trsnform), batch_size=batch_size, shuffle=True)\n","\n","    return train_loader, test_loader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HeFnFEUdJnTB","colab_type":"text"},"source":["## Create SGVLB loss"]},{"cell_type":"code","metadata":{"id":"_3fVDkE1JnTC","colab_type":"code","colab":{}},"source":["class SGVLB(nn.Module):\n","    def __init__(self, net, train_size):\n","        super(SGVLB, self).__init__()\n","        self.train_size = train_size # int, the len of dataset\n","        self.net = net # nn.Module\n","        \n","    def forward(self, input, target, kl_weight=1.0):\n","        \"\"\"\n","          input: is a torch.Tensor (a predictions of the model) \n","          target: is a torch.Tensor (a tensor of labels) \n","        \"\"\"\n","        assert not target.requires_grad\n","        kl = 0.0\n","        for module in self.net.children():\n","            if hasattr(module, 'kl_reg'):\n","                kl = kl + module.kl_reg()\n","                \n","        sgvlb_loss = # a scalar torch.Tensor, SGVLB loss\n","        return sgvlb_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1i_Q00ESJnTF","colab_type":"text"},"source":["## Define the model"]},{"cell_type":"code","metadata":{"id":"2ROeFVbJJnTF","colab_type":"code","colab":{}},"source":["model = Net(threshold=3)\n","optimizer = # optimizer\n","scheduler = # decrease learning rate by torch.optim.lr_scheduler\n","\n","logger = Logger('sparse_vd', fmt={\n","    'tr_loss': '3.1e',\n","    'te_loss': '3.1e',\n","    'sp_0':    '.3f',\n","    'sp_1':    '.3f',\n","    'sp_2':    '.3f',\n","    'lr':      '3.1e',\n","    'kl':      '.2f',\n","    'time':    '.2f',\n","})\n","\n","train_loader, test_loader = get_mnist(batch_size=100)\n","sgvlb = SGVLB(model, len(train_loader.dataset))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iMR2MYENJnTI","colab_type":"text"},"source":["## Train the model"]},{"cell_type":"code","metadata":{"id":"1gIcWJ2zJnTI","colab_type":"code","colab":{}},"source":["# here is a cpu version of the code\n","# the solution/colab file have a gpu version that is faster 11s/epoch insted of 30s/epoch now\n","# if you use gpu version, be sure your sgvlb loss and model are on gpu \n","\n","kl_weight = 0.02\n","epochs = 100\n","\n","for epoch in range(1, epochs + 1):\n","    time_start = time.perf_counter()\n","    scheduler.step()\n","    model.train()\n","    train_loss, train_acc = 0, 0 \n","    kl_weight = min(kl_weight+0.02, 1)\n","    logger.add_scalar(epoch, 'kl', kl_weight)\n","    logger.add_scalar(epoch, 'lr', scheduler.get_lr()[0])\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data = data.view(-1, 28*28)\n","        optimizer.zero_grad()\n","        \n","        output = model(data)\n","        pred = output.data.max(1)[1] \n","        loss = sgvlb(output, target, kl_weight)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        train_loss += float(loss) \n","        train_acc += np.sum(pred.numpy() == target.data.numpy())\n","\n","    logger.add_scalar(epoch, 'tr_loss', train_loss / len(train_loader.dataset))\n","    logger.add_scalar(epoch, 'tr_acc', train_acc / len(train_loader.dataset) * 100)\n","    \n","    model.eval()\n","    test_loss, test_acc = 0, 0\n","    for batch_idx, (data, target) in enumerate(test_loader):\n","        data = data.view(-1, 28*28)\n","        output = model(data)\n","        test_loss += float(sgvlb(output, target, kl_weight))\n","        pred = output.data.max(1)[1] \n","        test_acc += np.sum(pred.numpy() == target.data.numpy())\n","        \n","    logger.add_scalar(epoch, 'te_loss', test_loss / len(test_loader.dataset))\n","    logger.add_scalar(epoch, 'te_acc', test_acc / len(test_loader.dataset) * 100)\n","    \n","    for i, c in enumerate(model.children()):\n","        if hasattr(c, 'kl_reg'):\n","            logger.add_scalar(epoch, 'sp_%s' % i, (c.log_alpha.data.numpy() > model.threshold).mean())\n","    \n","    logger.add_scalar(epoch, 'time', time.perf_counter() - time_start)\n","    logger.iter_info()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1QxpeBe-JnTN","colab_type":"code","colab":{}},"source":["all_w, kep_w = 0, 0\n","\n","for c in model.children():\n","    kep_w += (c.log_alpha.data.numpy() < model.threshold).sum()\n","    all_w += c.log_alpha.data.numpy().size\n","\n","# compression_ratio should be > 30\n","compression_ratio = all_w/kep_w\n","print('compression_ratio =', compression_ratio)\n","assert compression_ratio > 30"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-nOBQsM6JnTR","colab_type":"text"},"source":["## Disk space"]},{"cell_type":"code","metadata":{"id":"QNSNrt8UJnTT","colab_type":"code","colab":{}},"source":["import scipy\n","import numpy as np\n","from scipy.sparse import csc_matrix, csc_matrix, coo_matrix, dok_matrix\n","\n","row, col, data = [], [], []\n","M = list(model.children())[0].mu.data.numpy()\n","LA = list(model.children())[0].log_alpha.data.numpy()\n","\n","for i in range(300):\n","    for j in range(28*28):\n","        if LA[i, j] < 3:\n","            row += [i]\n","            col += [j]\n","            data += [M[i, j]]\n","\n","Mcsr = csc_matrix((data, (row, col)), shape=(300, 28*28))\n","Mcsc = csc_matrix((data, (row, col)), shape=(300, 28*28))\n","Mcoo = coo_matrix((data, (row, col)), shape=(300, 28*28))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t8PKeFPLJnTV","colab_type":"code","colab":{}},"source":["np.savez_compressed('M_w', M)\n","scipy.sparse.save_npz('Mcsr_w', Mcsr)\n","scipy.sparse.save_npz('Mcsc_w', Mcsc)\n","scipy.sparse.save_npz('Mcoo_w', Mcoo)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpgaUhgvJnTX","colab_type":"code","colab":{}},"source":["!ls -lah | grep .npz "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wKMPx_8sJnTZ","colab_type":"text"},"source":["## Visualization"]},{"cell_type":"code","metadata":{"id":"_aEHgR9UJnTb","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import matplotlib as mpl\n","\n","from matplotlib import rcParams\n","rcParams['figure.figsize'] = 16, 4\n","rcParams['figure.dpi'] = 200\n","\n","\n","mask = (model.fc1.log_alpha.detach().numpy() < 3).astype(np.float)\n","W = model.fc1.mu.detach().numpy()\n","\n","# Normalize color map\n","max_val = np.max(np.abs(mask * W))\n","norm = mpl.colors.Normalize(vmin=-max_val,vmax=max_val)\n","\n","plt.imshow(mask * W, cmap='RdBu', interpolation=None, norm=norm)\n","plt.colorbar()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eBY9gsrmJnTd","colab_type":"code","colab":{}},"source":["s = 0\n","z = np.zeros((28*15, 28*15))\n","\n","for i in range(15):\n","    for j in range(15):\n","        s += 1\n","        z[i*28:(i+1)*28, j*28:(j+1)*28] = np.abs((mask * mu)[s].reshape(28, 28))\n","\n","plt.figure(figsize=(8, 5))\n","plt.imshow(z, cmap='hot_r')\n","plt.colorbar()\n","plt.axis('off')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_9cgWYVJnTg","colab_type":"text"},"source":["# Optional Research Assignment (up to 2 points)"]},{"cell_type":"markdown","metadata":{"id":"o9VHbVe8JnTh","colab_type":"text"},"source":["1. Study the model: \n","    - How sparsity and accuracy depend on maximum of KL-multiplier (kl_weight)?\n","    - How quality depends on the initialization of log_sigma (log_sigma)?\n","    - Study the KL approximation: what if we use the reparametrization trick to obtain an unbiased MC estimate of KL?\n","2. Compression:\n","    - What can we do to obtain better compression results with small quality degradation?\n","    - Propose and eval several options.\n","3. Study the Local reparametrization trick: \n","    - Does it really accelerate convergence?\n","    - Does variance of gradient decrease?\n","    \n","You can do one out of three parts. You need to provide evidence for results e.g., plots, etc."]},{"cell_type":"code","metadata":{"id":"BUC9yS-UJnTh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}